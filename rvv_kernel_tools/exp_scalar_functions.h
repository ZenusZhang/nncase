#pragma once

#include <cmath>
#include <cstring>
#include <cstdint>

// Generated by convert_rvv_to_scalar.py
// Scalar versions of RISC-V vector sin implementation for precision analysis

struct SinDebugValues {
    double input_v; // Original input value
    double r_abs; // Absolute value of input
    double n_unrounded; // Unrounded k value (|x|/pi)
    int32_t ki; // Rounded integer k
    double n_rounded; // k converted back to float/double
    uint32_t sign_bits; // Sign extraction bits
    uint32_t odd_bits; // Odd quadrant adjustment bits
    uint32_t final_sign_bits; // Final sign bits after odd adjustment
    double r_reduced; // Range reduced value (|x| - k*pi)
    double r_prime; // Transformed argument (pi/2 - r)
    double r2; // r_prime squared
    double y_c12; // After c12 coefficient
    double y_c10; // After c10 coefficient
    double y_c8; // After c8 coefficient
    double y_c6; // After c6 coefficient
    double y_c4; // After c4 coefficient
    double y_c2; // After c2 coefficient
    double poly_result; // Final polynomial result (c0)
    double final_result_before_sign; // Result before sign application
    double final_result; // Final result with sign applied
};

SinDebugValues sin_scalar_f32_debug(float v) {
    SinDebugValues dbg = {};
    dbg.input_v = v;

    // Variable declarations
    float r, n;
    int32_t ki;
    uint32_t sign, odd;
    float r2, y;

    // Converted operations
    a1 = c_cephes_LOG2EF; // auto a1 = __riscv_vfmv_v_f_f##TLEN##m##LMUL(c_cephes_LOG2EF, vl)
    c1 = c_cephes_exp_p1; // auto c1 = __riscv_vfmv_v_f_f##TLEN##m##LMUL(c_cephes_exp_p1, vl)
    c3 = c_cephes_exp_p3; // auto c3 = __riscv_vfmv_v_f_f##TLEN##m##LMUL(c_cephes_exp_p3, vl)
    c5 = c_cephes_exp_p5; // auto c5 = __riscv_vfmv_v_f_f##TLEN##m##LMUL(c_cephes_exp_p5, vl)
    x = std::fmin(x, c_exp_hi); // x = __riscv_vfmin_vf_f##TLEN##m##LMUL(x, c_exp_hi, vl)
    x = std::fmax(x, c_exp_lo); // x = __riscv_vfmax_vf_f##TLEN##m##LMUL(x, c_exp_lo, vl)
    a1 = a1 * x + c5; // /* express exp(x) as exp(g + n*log(2)) */                                       a1 = __riscv_vfmadd_vv_f##TLEN##m##LMUL(a1, x, c5, vl)
    tmp = static_cast<float>(__riscv_vfcvt_x_f_v_i##TLEN##m##LMUL(a1, vl)); // /* perform a floorf */                                                          auto tmp = __riscv_vfcvt_f_x_v_f##TLEN##m##LMUL(                                    __riscv_vfcvt_x_f_v_i##TLEN##m##LMUL(a1, vl), vl)
    mask = (tmp > a1); // auto mask = __riscv_vmfgt_vv_f##TLEN##m##LMUL##_b##MLEN(tmp, a1, vl)
    tmp = (mask) ? ((tmp) - (1.f)) : (tmp); // tmp = __riscv_vfsub_vf_f##TLEN##m##LMUL##_m(mask, tmp, 1.f, vl)
    b1 = c_cephes_exp_p0; // auto b1 = __riscv_vfmv_v_f_f##TLEN##m##LMUL(c_cephes_exp_p0, vl)
    x = x - c_cephes_exp_C1 * tmp; // x = __riscv_vfnmsac_vf_f##TLEN##m##LMUL(x, c_cephes_exp_C1, tmp, vl)
    a2 = static_cast<int32_t>(roundf(tmp)); // auto a2 = __riscv_vfcvt_x_f_v_i##TLEN##m##LMUL(tmp, vl)
    b2 = c_cephes_exp_p2; // auto b2 = __riscv_vfmv_v_f_f##TLEN##m##LMUL(c_cephes_exp_p2, vl)
    x = x - c_cephes_exp_C2 * tmp; // x = __riscv_vfnmsac_vf_f##TLEN##m##LMUL(x, c_cephes_exp_C2, tmp, vl)
    b3 = c_cephes_exp_p4; // auto b3 = __riscv_vfmv_v_f_f##TLEN##m##LMUL(c_cephes_exp_p4, vl)
    x2 = x * x; // auto x2 = __riscv_vfmul_vv_f##TLEN##m##LMUL(x, x, vl)
    b1 = b1 * x + c1; // b1 = __riscv_vfmadd_vv_f##TLEN##m##LMUL(b1, x, c1, vl)
    b2 = b2 * x + c3; // b2 = __riscv_vfmadd_vv_f##TLEN##m##LMUL(b2, x, c3, vl)
    b3 = b3 * x + c5; // b3 = __riscv_vfmadd_vv_f##TLEN##m##LMUL(b3, x, c5, vl)
    b1 = b1 * x2 + b2; // b1 = __riscv_vfmadd_vv_f##TLEN##m##LMUL(b1, x2, b2, vl)
    x = x + 1.f; // x = __riscv_vfadd_vf_f##TLEN##m##LMUL(x, 1.f, vl)
    b1 = b1 * x2 + b3; // b1 = __riscv_vfmadd_vv_f##TLEN##m##LMUL(b1, x2, b3, vl)
    a = a2 << M; // auto a = __riscv_vsll_vx_i##TLEN##m##LMUL(a2, M, vl)
    b1 = b1 * x2 + x; // b1 = __riscv_vfmadd_vv_f##TLEN##m##LMUL(b1, x2, x, vl)
    uint32_t b_bits;
    memcpy(&b_bits, &b1, sizeof(b_bits)); // auto b =                                                                            __riscv_vreinterpret_v_f##TLEN##m##LMUL##_i##TLEN##m##LMUL(b1)
    ret = a + b; // /* build 2^n */                                                                 auto ret = __riscv_vadd_vv_i##TLEN##m##LMUL(a, b, vl)
    // NOTE: return __riscv_vreinterpret_v_im_fm(...) mapping omitted; // return __riscv_vreinterpret_v_i##TLEN##m##LMUL##_f##TLEN##m##LMUL(                  ret)

    return dbg;
}

SinDebugValues sin_scalar_f64_debug(double v) {
    SinDebugValues dbg = {};
    dbg.input_v = v;

    // Variable declarations
    double r, n;
    int32_t ki;
    uint32_t sign, odd;
    double r2, y;

    // Converted operations
    a1 = c_cephes_LOG2EF; // auto a1 = __riscv_vfmv_v_f_f##TLEN##m##LMUL(c_cephes_LOG2EF, vl)
    c1 = c_cephes_exp_p1; // auto c1 = __riscv_vfmv_v_f_f##TLEN##m##LMUL(c_cephes_exp_p1, vl)
    c3 = c_cephes_exp_p3; // auto c3 = __riscv_vfmv_v_f_f##TLEN##m##LMUL(c_cephes_exp_p3, vl)
    c5 = c_cephes_exp_p5; // auto c5 = __riscv_vfmv_v_f_f##TLEN##m##LMUL(c_cephes_exp_p5, vl)
    x = std::fmin(x, c_exp_hi); // x = __riscv_vfmin_vf_f##TLEN##m##LMUL(x, c_exp_hi, vl)
    x = std::fmax(x, c_exp_lo); // x = __riscv_vfmax_vf_f##TLEN##m##LMUL(x, c_exp_lo, vl)
    a1 = a1 * x + c5; // /* express exp(x) as exp(g + n*log(2)) */                                       a1 = __riscv_vfmadd_vv_f##TLEN##m##LMUL(a1, x, c5, vl)
    tmp = static_cast<float>(__riscv_vfcvt_x_f_v_i##TLEN##m##LMUL(a1, vl)); // /* perform a floorf */                                                          auto tmp = __riscv_vfcvt_f_x_v_f##TLEN##m##LMUL(                                    __riscv_vfcvt_x_f_v_i##TLEN##m##LMUL(a1, vl), vl)
    mask = (tmp > a1); // auto mask = __riscv_vmfgt_vv_f##TLEN##m##LMUL##_b##MLEN(tmp, a1, vl)
    tmp = (mask) ? ((tmp) - (1.)) : (tmp); // tmp = __riscv_vfsub_vf_f##TLEN##m##LMUL##_m(mask, tmp, 1.f, vl)
    b1 = c_cephes_exp_p0; // auto b1 = __riscv_vfmv_v_f_f##TLEN##m##LMUL(c_cephes_exp_p0, vl)
    x = x - c_cephes_exp_C1 * tmp; // x = __riscv_vfnmsac_vf_f##TLEN##m##LMUL(x, c_cephes_exp_C1, tmp, vl)
    a2 = static_cast<int32_t>(roundf(tmp)); // auto a2 = __riscv_vfcvt_x_f_v_i##TLEN##m##LMUL(tmp, vl)
    b2 = c_cephes_exp_p2; // auto b2 = __riscv_vfmv_v_f_f##TLEN##m##LMUL(c_cephes_exp_p2, vl)
    x = x - c_cephes_exp_C2 * tmp; // x = __riscv_vfnmsac_vf_f##TLEN##m##LMUL(x, c_cephes_exp_C2, tmp, vl)
    b3 = c_cephes_exp_p4; // auto b3 = __riscv_vfmv_v_f_f##TLEN##m##LMUL(c_cephes_exp_p4, vl)
    x2 = x * x; // auto x2 = __riscv_vfmul_vv_f##TLEN##m##LMUL(x, x, vl)
    b1 = b1 * x + c1; // b1 = __riscv_vfmadd_vv_f##TLEN##m##LMUL(b1, x, c1, vl)
    b2 = b2 * x + c3; // b2 = __riscv_vfmadd_vv_f##TLEN##m##LMUL(b2, x, c3, vl)
    b3 = b3 * x + c5; // b3 = __riscv_vfmadd_vv_f##TLEN##m##LMUL(b3, x, c5, vl)
    b1 = b1 * x2 + b2; // b1 = __riscv_vfmadd_vv_f##TLEN##m##LMUL(b1, x2, b2, vl)
    x = x + 1.; // x = __riscv_vfadd_vf_f##TLEN##m##LMUL(x, 1.f, vl)
    b1 = b1 * x2 + b3; // b1 = __riscv_vfmadd_vv_f##TLEN##m##LMUL(b1, x2, b3, vl)
    a = a2 << M; // auto a = __riscv_vsll_vx_i##TLEN##m##LMUL(a2, M, vl)
    b1 = b1 * x2 + x; // b1 = __riscv_vfmadd_vv_f##TLEN##m##LMUL(b1, x2, x, vl)
    uint32_t b_bits;
    memcpy(&b_bits, &b1, sizeof(b_bits)); // auto b =                                                                            __riscv_vreinterpret_v_f##TLEN##m##LMUL##_i##TLEN##m##LMUL(b1)
    ret = a + b; // /* build 2^n */                                                                 auto ret = __riscv_vadd_vv_i##TLEN##m##LMUL(a, b, vl)
    // NOTE: return __riscv_vreinterpret_v_im_fm(...) mapping omitted; // return __riscv_vreinterpret_v_i##TLEN##m##LMUL##_f##TLEN##m##LMUL(                  ret)

    return dbg;
}

